{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18BCE2522"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=\"You'll come out being able to visualize word frequency distributions of any novel that you can find on Project Gutenberg.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"You'll\", 'come', 'out', 'being', 'able', 'to', 'visualize', 'word', 'frequency', 'distributions', 'of', 'any', 'novel', 'that', 'you', 'can', 'find', 'on', 'Project', 'Gutenberg.']\n"
     ]
    }
   ],
   "source": [
    "dd=list(ss.split())\n",
    "print(dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For multiple sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"You'll come out being able to visualize word frequency distributions of any novel that you can find on Project Gutenberg. The NLP skills you develop, however, will be applicable to much of the data that data scientists encounter as the vast proportion of the world's data is unstructured data and includes a great deal of text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"You'll\", 'come', 'out', 'being', 'able', 'to', 'visualize', 'word', 'frequency', 'distributions', 'of', 'any', 'novel', 'that', 'you', 'can', 'find', 'on', 'Project', 'Gutenberg.', 'The', 'NLP', 'skills', 'you', 'develop,', 'however,', 'will', 'be', 'applicable', 'to', 'much', 'of', 'the', 'data', 'that', 'data', 'scientists', 'encounter', 'as', 'the', 'vast', 'proportion', 'of', 'the', \"world's\", 'data', 'is', 'unstructured', 'data', 'and', 'includes', 'a', 'great', 'deal', 'of', 'text.']\n"
     ]
    }
   ],
   "source": [
    "d=list(s.split())\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"stopwords\")\n",
    "#nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sent=\"Web mining is the application of data mining techniques to discover patterns from the World Wide Web. As the name proposes, this is information gathered by mining the web. It makes utilization of automated apparatuses to reveal and extricate data from servers and web2 reports, and it permits organizations to get to both organized and unstructured information from browser activities, server logs, website and link structure, page content and different sources.The goal of Web structure mining is to generate structural summary about the Web site and Web page.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words=['.',',','a','they','the','his','so','and','were','from','that','of','in','only','with','to']\n",
    "word_tokens=word_tokenize(example_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_sentences=[w for w in word_tokens if not w in stop_words]\n",
    "# OR\n",
    "filtered_sentences=[]\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentences.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f92e1b7d801f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"From which companies do you experience the best customer service aside from @zappos and @apple?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtext_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtokens_without_sw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_tokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"From which companies do you experience the best customer service aside from @zappos and @apple?\"\n",
    "text_tokens = word_tokenize(text)\n",
    "\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "\n",
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after removing stopwords-\n",
      "\n",
      "\n",
      "['Web', 'mining', 'is', 'application', 'data', 'mining', 'techniques', 'discover', 'patterns', 'World', 'Wide', 'Web', 'As', 'name', 'proposes', 'this', 'is', 'information', 'gathered', 'by', 'mining', 'web', 'It', 'makes', 'utilization', 'automated', 'apparatuses', 'reveal', 'extricate', 'data', 'servers', 'web2', 'reports', 'it', 'permits', 'organizations', 'get', 'both', 'organized', 'unstructured', 'information', 'browser', 'activities', 'server', 'logs', 'website', 'link', 'structure', 'page', 'content', 'different', 'sources.The', 'goal', 'Web', 'structure', 'mining', 'is', 'generate', 'structural', 'summary', 'about', 'Web', 'site', 'Web', 'page']\n"
     ]
    }
   ],
   "source": [
    "print(\"Text after removing stopwords-\")\n",
    "print(\"\\n\")From which companies do you experience the best customer service aside from @zappos and @apple?\n",
    "print(filtered_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1=open(\"D:/Downloads/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am joining web mining class, with anaconda.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line=file1.read()\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'joining', 'web', 'mining', 'class,', 'with', 'anaconda.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=line.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_file=open(\"D:/Downloads/filtered_text.txt\",'a')\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        append_file.write(w+\" \")\n",
    "append_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens=word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'joining', 'web', 'mining', 'class', ',', 'with', 'anaconda', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
