{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18BCE2522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PACKAGES\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim import corpora, similarities\n",
    "from gensim.models import LsiModel\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to filter out stopwords and apply word stemming\n",
    "def filter_words_and_get_word_stems(document, word_tokenizer, word_stemmer,\n",
    "                                    stopword_set, pattern_to_match_words=r\"[^\\w]\",\n",
    "                                    word_length_minimum_n_chars=2):\n",
    "    \"\"\"Remove multiple white spaces and all non word content from text and\n",
    "    extract words. Then filter out stopwords and words with a length smaller\n",
    "    than word_length_minimum and apply word stemmer to get wordstems. Finally\n",
    "    return word stems.\n",
    "    \"\"\"\n",
    "    document = re.sub(pattern_to_match_words, r\" \", document)\n",
    "    document = re.sub(r\"\\s+\", r\" \", document)\n",
    "    words = word_tokenizer.tokenize(document)\n",
    "    words_filtered = [word.lower()\n",
    "                      for word in words\n",
    "                      if word.lower() not in stopword_set and len(word) >= word_length_minimum_n_chars]\n",
    "    word_stems = [word_stemmer.lemmatize(word) for word in words_filtered]\n",
    "    return(word_stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training text data to calculate TF-IDF model from\n",
    "documents_train = [\n",
    "  \"She goes to school.\",\n",
    "  \"She runs to the shop.\",\n",
    "  \"I go to school and he goes to the shop.\"]\n",
    "\n",
    "#test data text data to match \n",
    "document_test = \"He runs to school.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Stems of Training Documents: [['go', 'school'], ['run', 'shop'], ['go', 'school', 'go', 'shop']]\n",
      "\n",
      "Word Stems of Test Document: ['run', 'school']\n"
     ]
    }
   ],
   "source": [
    "#set stopword set, word stemmer and word tokenizer\n",
    "stopword_set = set(stopwords.words(\"english\"))\n",
    "word_tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "word_stemmer =  nltk.WordNetLemmatizer()\n",
    "\n",
    "#apply cleaning, filtering and word stemming to training documents\n",
    "word_stem_arrays_train = [\n",
    "        filter_words_and_get_word_stems(\n",
    "                str(document),\n",
    "                word_tokenizer,\n",
    "                word_stemmer,\n",
    "                stopword_set\n",
    "                ) for document in documents_train]\n",
    "print(\"Word Stems of Training Documents:\", word_stem_arrays_train)\n",
    "print(\"\")\n",
    "\n",
    "#apply cleaning, filtering and word stemming to test document\n",
    "word_stem_array_test = filter_words_and_get_word_stems(\n",
    "        document_test,\n",
    "        word_tokenizer,\n",
    "        word_stemmer,\n",
    "        stopword_set)\n",
    "print(\"Word Stems of Test Document:\", word_stem_array_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary : Dictionary(4 unique tokens: ['go', 'school', 'run', 'shop'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create dictionary containing unique word stems of training documents\n",
    "#TF (term frequencies) or \"global weights\"\n",
    "dictionary = corpora.Dictionary(\n",
    "  word_stem_array_train\n",
    "  for word_stem_array_train in word_stem_arrays_train)\n",
    "print(\"Dictionary :\", dictionary)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus : [[(0, 1), (1, 1)], [(2, 1), (3, 1)], [(0, 2), (1, 1), (3, 1)]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create corpus containing word stem id from dictionary and word stem count\n",
    "#for each word in each document\n",
    "#DF (document frequencies, for all terms in each document) or \"local weights\"\n",
    "corpus = [\n",
    "  dictionary.doc2bow(word_stem_array_train)\n",
    "  for word_stem_array_train in word_stem_arrays_train]\n",
    "print(\"Corpus :\", corpus)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivation of Term Matrix T of Training Document Word Stems: \n",
      " [[ 0.79411857  0.47930412  0.05482989  0.36964434]\n",
      " [-0.2236068  -0.2236068   0.67082039  0.67082039]\n",
      " [-0.26339267  0.68576057  0.54497127 -0.40418197]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create LSI model (Latent Semantic Indexing) from corpus and dictionary\n",
    "#LSI model consists of Singular Value Decomposition (SVD) of\n",
    "#Term Document Matrix M: M = T x S x D'\n",
    "#and dimensionality reductions of T, S and D (\"Derivation\")\n",
    "lsi_model = LsiModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary #, num_topics = 2 #(opt. setting for explicit dim. change)\n",
    "        )\n",
    "print(\"Derivation of Term Matrix T of Training Document Word Stems: \\n\",\n",
    "      lsi_model.get_topics())\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSI Vectors of Training Document Word Stems: \n",
      " [[(0, 1.2734226922603291), (1, -0.447213595499958), (2, 0.42236790460030676)], [(0, 0.4244742307534435), (1, 1.3416407864998736), (2, 0.14078930153343583)], [(0, 2.4371856025006933), (2, -0.24520672699445256)]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Derivation of Term Document Matrix of Training Document Word Stems = M' x [Derivation of T]\n",
    "print(\"LSI Vectors of Training Document Word Stems: \\n\",\n",
    "      [lsi_model[document_word_stems] for document_word_stems in corpus])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarities of LSI Vectors of Training Documents: \n",
      " [array([ 1.0000000e+00, -1.9446199e-08,  8.6602545e-01], dtype=float32), array([-1.9446199e-08,  1.0000000e+00,  2.8867516e-01], dtype=float32), array([0.86602545, 0.28867516, 1.        ], dtype=float32)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#calculate cosine similarity matrix for all training document LSI vectors\n",
    "cosine_similarity_matrix = similarities.MatrixSimilarity(lsi_model[corpus])\n",
    "print(\"Cosine Similarities of LSI Vectors of Training Documents: \\n\",\n",
    "      [row for row in cosine_similarity_matrix])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSI Vector Test Document: [(0, 0.5341340127734087), (1, 0.44721359549995776), (2, 1.230731837728502)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#calculate LSI vector from word stem counts of the test document and the LSI model content\n",
    "vector_lsi_test = lsi_model[dictionary.doc2bow(word_stem_array_test)]\n",
    "print(\"LSI Vector Test Document:\", vector_lsi_test)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarities of Test Document LSI Vectors to Training Documents LSI Vectors:\n",
      " [0.49999997 0.5        0.28867513]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#perform a similarity query against the corpus\n",
    "cosine_similarities_test = cosine_similarity_matrix[vector_lsi_test]\n",
    "print(\"Cosine Similarities of Test Document LSI Vectors to Training Documents LSI Vectors:\\n\",\n",
    "      cosine_similarities_test)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar Training Document to Test Document:\n",
      " She runs to the shop.\n"
     ]
    }
   ],
   "source": [
    "#get text of test documents most similar training document\n",
    "most_similar_document_test = documents_train[np.argmax(cosine_similarities_test)]\n",
    "print(\"Most similar Training Document to Test Document:\\n\", most_similar_document_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
